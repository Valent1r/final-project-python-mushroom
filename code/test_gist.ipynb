{
 "cells": [
  

{
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install webdriver_manager\n",
    "!pip install Image"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extracts information about mushrooms from the website http://www.mushroom.world. \n",
"-1.Uses the BeautifulSoup library (from bs4 import BeautifulSoup) to perform HTML analysis of web pages, and the modules requests, re, json, and urljoin to make HTTP requests, handle regular expressions, process JSON data, and manage URLs, respectively. \n",
"\n",
"-2.Scrape_mushroom_list(url) takes a URL as a parameter, retrieves the HTML content of the page, finds all links pointing to individual mushroom pages, and then uses the function scrape_mushroom(url) to extract specific information about each mushroom. \n",
"\n",
"-3.The function scrape_mushroom(url) retrieves information such as the name, labels, texts, and images for a specific mushroom from its individual page. \n",
"\n",
"-4.The script uses a dictionary (edibility_dict) to store information about the edibility of mushrooms, where 'c' stands for edible, and 'p' stands for poisonous. \n",
"\n",
"-5.The edibility information for each mushroom extracted from the web page is removed since the value is empty, and the edibility is added to the final output each extracted mushroom. \n",
"\n",
"###Script displays information about all mushrooms in an indented JSON string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
"import requests \n",
"import re \n",
"import json \n",
"from urllib.parse import urljoin \n",
"\n",
"def scrape_mushroom_list(url): \n",
    "data = requests.get(url).text \n",
    "soup = BeautifulSoup(data, 'html.parser') \n",
    "\n",
    "# Find all links to individual mushroom pages \n",
    "mushroom_links = soup.find_all('a', href=re.compile(r'/show?n=')) \n",
    "mushroom_urls = [urljoin('http://www.mushroom.world', link['href']) for link in mushroom_links] \n",
    "\n",
    "# Retrieve information for each mushroom\n",
    "mushrooms = [scrape_mushroom(link) for link in mushroom_urls] \n",
    "\n",
 "return mushrooms"
   ]
  },


{
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def scrape_mushroom(url): \n",
    "data = requests.get(url).text \n",
    "soup = BeautifulSoup(data, 'html.parser') \n",
"\n",
    "# Extract specific information for an individual mushroom \n",
    "name_content = soup.find(class_='caption').find('b').contents \n",
    "names = re.sub('[^A-Za-z0-9( ]+', '', name_content[0])).split('(')) \n",
    "names = [n.strip() for n in names] \n",
    "name1 = names[0] \n",
    "name2 = names[1] if len(names) > 1 else '' \n",
"\n",
    "labels = soup.find_all(class_='labelus') \n",
    "labels = [label.contents[0] for label in labels] \n",
"\n",
    "texts = soup.find_all(class_='textus') \n",
    "texts = [text.contents[0] for text in texts] \n",
"\n",
    "# Updated code to remove unwanted lines \n",
    "description = soup.find(class_='longtextus') \n",
    "if description: \n",
        "unwanted_links = description.find_all('a', href=True) \n",
        "for link in unwanted_links: \n",
            "link.extract()  # Remove unwanted links \n",
"\n",
        "description = description.get_text(separator=' ', strip=True) \n",
    "else: \n",
        "description = 'Description not available' \n",
"\n",
    "texts.append(description) \n",
    "assert len(labels) == len(texts) \n",
"\n",
    "images = soup.find(id='mushroom-list').find_all(class_='image') \n",
    "image_urls = [urljoin('http://www.mushroom.world', image.a['href']) for image in images] \n",
"\n",
    "mushroom = dict(name1=name1, name2=name2, images=image_urls, info=dict()) \n",
"\n",
    "for i in range(len(labels)): \n",
        "mushroom['info'][labels[i]] = texts[i] \n",
"\n",
    "return mushroom"

   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the `command` mode shortcuts from within the Notebook Application itself. \n",
    "\n",
    "Head to the **Settings** menu and select the **Settings Editor** item.\n",
    "A dialog will guide you through the process of adding custom keyboard shortcuts.\n",
    "\n",
    "Keyboard shortcut set from within the Notebook Application will be persisted to your configuration file. \n",
    "A single action may have several shortcuts attached to it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
